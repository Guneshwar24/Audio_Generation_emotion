{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AUDIO GENERATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from audioGan import AudioGAN\n",
    "from ganSetup import GANConfig\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "config = GANConfig()\n",
    "myGan = AudioGAN(label = config.LABEL, load= True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUTOENCODER PLOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX = 11\n",
    "sample = myGan.testData[INDEX]\n",
    "\n",
    "print(\"Original:\")\n",
    "IPython.display.display(ipd.Audio(data=sample, rate=config.SAMPLE_RATE))\n",
    "sf.write(config.AUTO_ENCODER_PATH + \"AE_Original_test_\" + config.LABEL + \".wav\", sample, config.SAMPLE_RATE)\n",
    "\n",
    "result = myGan.autoencoder.predict(sample.reshape((1, config.AUDIO_SHAPE))).flatten()\n",
    "\n",
    "sf.write(\"AE_output.wav\", result, 16000)\n",
    "\n",
    "\n",
    "print(\"Result:\")\n",
    "IPython.display.display(ipd.Audio(data=result, rate=config.SAMPLE_RATE))\n",
    "sf.write(config.AUTO_ENCODER_PATH + \"AE_Generated_test_\" + config.LABEL + \".wav\", result, config.SAMPLE_RATE)\n",
    "\n",
    "fig = plt.figure(figsize=(15, 4))\n",
    "Original = fig.add_subplot(1, 2, 1)\n",
    "Result = fig.add_subplot(1, 2, 2)\n",
    "\n",
    "Original.set_title('Original')\n",
    "Result.set_title('Result')\n",
    "\n",
    "Original.plot(sample, '.', color='blue')\n",
    "Result.plot(result, '.', color='green')\n",
    "\n",
    "plt.subplots_adjust(wspace=0.2, hspace=0.3)\n",
    "plt.show()\n",
    "\n",
    "fig.savefig(config.PICTURE_PATH + \"AE_Compare_test_\" + config.LABEL + \".png\", bbox_inches=\"tight\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GENERATOR PLOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "import soundfile as sf  # Assuming you have the soundfile library installed\n",
    "import os\n",
    "\n",
    "# Load models\n",
    "encoder_path = os.path.join(myGan.model_path, 'encoder.keras')\n",
    "encoder = load_model(encoder_path)\n",
    "generator_path = os.path.join(myGan.model_path, 'generator.keras')\n",
    "generator = load_model(generator_path)\n",
    "\n",
    "# Function to predict using encoder and generator\n",
    "def generate_audio_from_real(real_audio):\n",
    "    # Assuming real_audio is preprocessed and shaped correctly [(1, audio_length)]\n",
    "    latent_vector = encoder.predict(real_audio.reshape(1, -1))\n",
    "    generated_audio = generator.predict(latent_vector).flatten()\n",
    "\n",
    "    # Display and save the audio\n",
    "    print(\"Original Audio:\")\n",
    "    ipd.display(ipd.Audio(data=real_audio.flatten(), rate=config.SAMPLE_RATE))\n",
    "    sf.write(config.AUTO_ENCODER_PATH + \"Gen_Original_test_\" + config.LABEL + \".wav\", real_audio.flatten(), config.SAMPLE_RATE)\n",
    "\n",
    "    print(\"Generated Audio:\")\n",
    "    ipd.display(ipd.Audio(data=generated_audio, rate=config.SAMPLE_RATE))\n",
    "    sf.write(config.AUTO_ENCODER_PATH + \"Gen_Generated_test_\" + config.LABEL + \".wav\", generated_audio, config.SAMPLE_RATE)\n",
    "\n",
    "    # Plot the original and generated audio waveforms\n",
    "    fig = plt.figure(figsize=(15, 4))\n",
    "    Original = fig.add_subplot(1, 2, 1)\n",
    "    Result = fig.add_subplot(1, 2, 2)\n",
    "\n",
    "    Original.set_title('Original')\n",
    "    Result.set_title('Generated')\n",
    "\n",
    "    Original.plot(real_audio.flatten(), '.', color='blue')\n",
    "    Result.plot(generated_audio, '.', color='green')\n",
    "\n",
    "    plt.subplots_adjust(wspace=0.2, hspace=0.3)\n",
    "    plt.show()\n",
    "\n",
    "    # Save the plot\n",
    "    fig.savefig(config.PICTURE_PATH + \"Gen_Compare_test_\" + config.LABEL + \".png\", bbox_inches=\"tight\")\n",
    "\n",
    "    return generated_audio\n",
    "\n",
    "# Example usage\n",
    "INDEX = 10\n",
    "real_audio = myGan.testData[INDEX]  # Replace this with an actual audio sample\n",
    "generated_audio = generate_audio_from_real(real_audio)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frechet Audio Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from frechet_audio_distance import FrechetAudioDistance\n",
    " \n",
    "# Create an instance of FrechetAudioDistance\n",
    "frechet = FrechetAudioDistance(\n",
    "    model_name=\"vggish\",   # Options: \"vggish\", \"pann\", \"clap\", \"encodec\"\n",
    "    sample_rate=16000,     # Sample rate of the audio files\n",
    "    use_pca=False,         # Use PCA for dimensionality reduction\n",
    "    use_activation=False,  # Use activations for computing distance\n",
    "    verbose=False          # Verbosity mode\n",
    ")\n",
    " \n",
    "# Define the paths to the original and generated audio files\n",
    "original_audio_path = 'WavFiles/Autoencoder/AE_Original_test_fname.wav'\n",
    "generated_audio_path = 'WavFiles/Autoencoder/AE_Generated_test_fname.wav'\n",
    " \n",
    "# Compute the Frechet Audio Distance score\n",
    "fad_score = frechet.score(original_audio_path, generated_audio_path, dtype=\"float16\")\n",
    " \n",
    "# Print the FAD score\n",
    "print(f\"The Frechet Audio Distance (FAD) is: {fad_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spectral Convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "\n",
    "# Function to compute the spectrogram of a wav file\n",
    "def compute_spectrogram(wav_file, sr=16000, n_fft=2048, hop_length=512):\n",
    "    y, sr = librosa.load(wav_file, sr=sr)\n",
    "    S = librosa.stft(y, n_fft=n_fft, hop_length=hop_length)\n",
    "    S_db = librosa.amplitude_to_db(np.abs(S), ref=np.max)\n",
    "    return S_db\n",
    "\n",
    "# Function to calculate Spectral Convergence\n",
    "def calculate_spectral_convergence(real_spectrogram, generated_spectrogram):\n",
    "    assert real_spectrogram.shape == generated_spectrogram.shape, \"Spectrograms must have the same shape\"\n",
    "    diff_norm = np.linalg.norm(real_spectrogram - generated_spectrogram, 'fro')\n",
    "    real_norm = np.linalg.norm(real_spectrogram, 'fro')\n",
    "    sc = diff_norm / real_norm\n",
    "    return sc\n",
    "\n",
    "# Define the paths to the original and generated audio files\n",
    "original_audio_path = 'WavFiles/Autoencoder/AE_Original_test_fname.wav'\n",
    "generated_audio_path = 'WavFiles/Autoencoder/AE_Generated_test_fname.wav'\n",
    "\n",
    "# Compute spectrograms for the original and generated audio files\n",
    "real_spectrogram = compute_spectrogram(original_audio_path)\n",
    "generated_spectrogram = compute_spectrogram(generated_audio_path)\n",
    "\n",
    "# Calculate Spectral Convergence\n",
    "sc_score = calculate_spectral_convergence(real_spectrogram, generated_spectrogram)\n",
    "\n",
    "# Print the SC score\n",
    "print(f\"The Spectral Convergence (SC) is: {sc_score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
